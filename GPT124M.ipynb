{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzOshnYOpqP_"
      },
      "source": [
        "# NanoGPT\n",
        "\n",
        "Training a decoder-only model (NanoGPT) to genereate text in Shakespear stype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J93p7rk7qK-P"
      },
      "source": [
        "## Install Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8MnYOQ4xcKXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ba29a0-dee2-434f-8a16-b84aa7506fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Tiktoken for tokenization\n",
        "!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHuCVeKYqWvo"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "igAl5bXSqZWo"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Third-Party Imports\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QNqCz2fqSX0"
      },
      "source": [
        "## Transformer Achitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jgXGx_-YqVNH"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Xev9Ycq7Qe"
      },
      "source": [
        "## Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "27h8bNasq-Xe"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024  # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12       # number of layers\n",
        "    n_head: int = 12        # number of heads\n",
        "    n_embd: int = 768       # embedding dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHpQZ_avrMyd"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NcXgug8hrPO-"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B*T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0eEHPNrrz6b"
      },
      "source": [
        "## Device Configutration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w_OS901rr3SO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76341e2c-2d90-4003-a0aa-fe957cc4cbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Total GPU memory: 15.84 GB\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Add CUDA check and memory info\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Modify the device selection\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veEWTXgVsIVz"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjuBy6HGb9Ta",
        "outputId": "72253b32-c302-49ba-a538-c09bebffed29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total model parameters: 124,439,808\n",
            "\n",
            "loaded 338025 tokens\n",
            "1 epoch = 82 batches\n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 100 | loss: 5.6404\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 200 | loss: 5.5134\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 300 | loss: 5.3910\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 400 | loss: 5.2524\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 500 | loss: 4.6032\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 600 | loss: 4.9332\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 700 | loss: 4.6553\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 800 | loss: 4.7186\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 900 | loss: 4.1250\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,000 | loss: 4.4559\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,100 | loss: 4.5999\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 1,200 | loss: 4.1833\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,300 | loss: 3.9645\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,400 | loss: 3.8615\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 1,500 | loss: 4.2542\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,600 | loss: 4.2003\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,700 | loss: 4.0364\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 1,800 | loss: 3.5748\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 1,900 | loss: 3.8551\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,000 | loss: 3.6285\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,100 | loss: 3.6612\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,200 | loss: 3.5561\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,300 | loss: 3.6639\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,400 | loss: 3.2081\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,500 | loss: 3.3870\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,600 | loss: 3.5052\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,700 | loss: 3.1396\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,800 | loss: 3.2755\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 2,900 | loss: 3.1607\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 3,000 | loss: 2.9122\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,100 | loss: 2.9336\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,200 | loss: 2.9337\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,300 | loss: 2.6723\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,400 | loss: 2.7260\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 3,500 | loss: 2.9506\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,600 | loss: 2.5892\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,700 | loss: 2.6180\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,800 | loss: 2.6281\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 3,900 | loss: 2.5213\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,000 | loss: 2.1585\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,100 | loss: 2.2888\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,200 | loss: 2.1620\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,300 | loss: 2.1423\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,400 | loss: 2.1547\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,500 | loss: 2.1725\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,600 | loss: 1.8683\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,700 | loss: 2.0018\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,800 | loss: 1.9190\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 4,900 | loss: 1.8945\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,000 | loss: 1.5110\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,100 | loss: 1.7640\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,200 | loss: 1.6642\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,300 | loss: 1.4798\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,400 | loss: 1.3588\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,500 | loss: 1.3964\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,600 | loss: 1.4300\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,700 | loss: 1.2615\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,800 | loss: 1.1194\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 5,900 | loss: 1.0589\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,000 | loss: 1.1983\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,100 | loss: 1.0443\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,200 | loss: 0.9556\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,300 | loss: 0.8669\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,400 | loss: 0.8857\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,500 | loss: 0.7104\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 6,600 | loss: 0.6660\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,700 | loss: 0.5840\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,800 | loss: 0.4949\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 6,900 | loss: 0.5185\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,000 | loss: 0.4455\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,100 | loss: 0.4218\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 7,200 | loss: 0.4893\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,300 | loss: 0.4808\n",
            "    \n",
            "GPU Memory: 2.92GB / 12.74GB\n",
            "step 7,400 | loss: 0.4370\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,500 | loss: 0.3230\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,600 | loss: 0.3644\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,700 | loss: 0.3395\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,800 | loss: 0.2555\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 7,900 | loss: 0.1981\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 8,000 | loss: 0.1642\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 8,100 | loss: 0.1976\n",
            "    \n",
            "GPU Memory: 3.75GB / 12.74GB\n",
            "step 8,200 | loss: 0.1687\n",
            "    \n",
            "\n",
            "Reached target loss! Final loss: 0.0882 at step 8,251\n",
            "Model saved to gpt_model.pt\n"
          ]
        }
      ],
      "source": [
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Model\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "# Print total model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {total_params:,}\\n\")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoaderLite(B = 4, T = 1024)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
        "\n",
        "step = 1\n",
        "while True:\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if device == 'cuda' and step % 100 == 0:  # Print memory every 100 steps\n",
        "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB / {torch.cuda.memory_reserved() / 1e9:.2f}GB\")\n",
        "\n",
        "    if step % 100 == 0:  # Print details every 100 steps\n",
        "        print(f'step {step:,} | loss: {loss.item():.4f}')\n",
        "        print(\"    \")\n",
        "\n",
        "    if loss.item() < 0.09:\n",
        "        print(f'\\nReached target loss! Final loss: {loss.item():.4f} at step {step:,}')\n",
        "        save_path = 'gpt_model.pt'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "        break\n",
        "\n",
        "    step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZ4Yk9LQehd"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ByO3iW55Qehd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c94ddb-3196-4ee7-8a5e-33f94d09a2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total model parameters: 124,439,808\n",
            "Model saved to nano_gpt_model.pt\n"
          ]
        }
      ],
      "source": [
        "# Print total model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "# Save the model\n",
        "save_path = 'nano_gpt_model.pt'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj9Rs-dysuOg"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CE2_CV1TcttD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb8608f-138c-4b3a-bc82-204c41490045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating text samples...\n",
            "\n",
            "Generated text:\n",
            "\n",
            "Once upon a time Bol sorrow did man sp;\n",
            "First patience the court\n",
            "\n",
            "SLYry; I may stay that been resARERSAS Mician:\n",
            "Vols of Somerset, hoiestPut by his I besiege!\n",
            "Then, as I am as yet he hath been in Padua but the world:\n",
            "\n",
            "To look from him.\n",
            "With joyful for these sport in some cause:\n",
            "Of this same, by your honour and not, and daughter, young Warwick in\n"
          ]
        }
      ],
      "source": [
        "# STOP\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "# Text generation\n",
        "print(\"\\nGenerating text samples...\")\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "context = \"Once upon a time\"\n",
        "x = torch.tensor([enc.encode(context)], dtype=torch.long, device=device)\n",
        "\n",
        "max_length = 100  # Generate 100 tokens\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits = model(x)[0]\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat([x, xcol], dim=1)\n",
        "\n",
        "print(\"\\nGenerated text:\")\n",
        "tokens = x[0].tolist()  # Take first sequence\n",
        "decoded = enc.decode(tokens)\n",
        "print(f\"\\n{decoded}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}